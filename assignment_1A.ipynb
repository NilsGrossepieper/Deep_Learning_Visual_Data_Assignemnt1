{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilsGrossepieper/Deep_Learning_Visual_Data_Assignemnt1/blob/main/assignment_1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGmGhEhc-9tv"
      },
      "source": [
        "# PyTorch Tensors\n",
        "\n",
        "In order to get anything done with deep learning, we need some way to store and manipulate data. \n",
        "Therefore, to start, we develop proficiency with the n-dimensional array in PyTorch, which is also called the tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ditQAfhTXy3l"
      },
      "outputs": [],
      "source": [
        "# I import all necessary packages at the beginning\n",
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDOSz3QVaSbe"
      },
      "source": [
        "### Exercise 1A:\n",
        "Create in the next code cell a 32-bit floating point torch Tensor of shape `(4,5)` which is filled with square root of 29."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PTTRn9HFeG9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3cfad8-cda0-4b47-f7dc-51928e5ecc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5.3852, 5.3852, 5.3852, 5.3852, 5.3852],\n",
            "        [5.3852, 5.3852, 5.3852, 5.3852, 5.3852],\n",
            "        [5.3852, 5.3852, 5.3852, 5.3852, 5.3852],\n",
            "        [5.3852, 5.3852, 5.3852, 5.3852, 5.3852]]) torch.float32\n"
          ]
        }
      ],
      "source": [
        "# First I create the tensor and its shape, in the next steps I addapt the tensor \n",
        "# to the task\n",
        "torch_1 = torch.ones((4, 5))\n",
        "torch_1[:, :] = math.sqrt(29)\n",
        "torch_1 = torch_1.to(torch.float32)\n",
        "\n",
        "print(torch_1, torch_1.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxQKhClJ427n"
      },
      "source": [
        "### Exercise 1B:\n",
        "\n",
        "Convert the tensor created in the previous exercise to 64-bit floating point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gTnfu78t5OTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28e9366-28de-4471-cb1b-bc5757ea7e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float64\n"
          ]
        }
      ],
      "source": [
        "# Change the data type\n",
        "torch_1 = torch_1.to(torch.float64)\n",
        "\n",
        "print(torch_1.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Teyrxoa-bl"
      },
      "source": [
        "### Exercise 2:\n",
        "Create a tensor `x` (you can give it the shape you want) such that its elements are sampled from a normal distribution of mean 3 and variance 2.\n",
        "\n",
        "Next, create a tensor `y` having the same shape as `x` such that its elements are sampled from a uniform distribution between -1 and 2. Your code should be robust to the shape of `x` such that if you change the shape of `t`, the shape of your `y` should adapt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IQVraHuJa9yJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0595615e-a1ee-40f1-bf71-7f71cf17a639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.0012,  1.3123,  9.4443,  6.6869],\n",
            "        [ 1.8976,  1.3646,  5.4515,  3.0240],\n",
            "        [ 2.6429,  5.1444, -5.3690,  4.4211]])\n",
            "tensor([[ 1.7811, -0.1711,  1.3829,  1.2669],\n",
            "        [-0.5189,  1.6116,  0.5296, -0.4119],\n",
            "        [ 1.0384, -0.4832,  0.8416,  0.1425]])\n"
          ]
        }
      ],
      "source": [
        "# First I set up the shape for x and y, the I create x and y respectively \n",
        "x_shape = (3, 4)\n",
        "x = torch.normal(mean = 3, std = 4, size = x_shape)\n",
        "# since std^2=var and 2^2=4 I can also write the distribution this way\n",
        "y = torch.empty_like(x).uniform_(-1, 2)\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEdNH8f_aMaq"
      },
      "source": [
        "### Exercise 3:\n",
        "\n",
        "Run `A / A.sum(axis=1)` for the following tensor A and see what happens. Can you analyze the reason? Create a text cell to provide your answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NkW8P7eRaU_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9583cb-0590-465b-ca2d-f403f76fa625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
            "        [ 5.,  6.,  7.,  8.,  9.],\n",
            "        [10., 11., 12., 13., 14.],\n",
            "        [15., 16., 17., 18., 19.]]) torch.Size([4, 5])\n",
            "tensor([10., 35., 60., 85.]) torch.Size([4])\n",
            "tensor([[10.],\n",
            "        [35.],\n",
            "        [60.],\n",
            "        [85.]]) torch.Size([4])\n"
          ]
        }
      ],
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(4, 5)\n",
        "\n",
        "# First I take a look at A, A.sum(axis=1) and A / A.sum(axis=1)\n",
        "B = A.sum(axis=1)\n",
        "print(A, A.shape)\n",
        "print(B, B.shape)\n",
        "print(B.reshape(4, 1), B.shape)\n",
        "#print(A / B)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run the last line of code you get the following error message: \"The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1\" The proble is that the tensor A is a two dimensional tensor, while the tensor B (A.sum(axis=1)) is only a single dimensional tensor. Since this dimension is not a singleton dimension (size greater than 1), the tensors can not be broadcasted to match their sizes for this dimension. Since the two tensors have not the same dimensions we can not perform the computation A / A.sum(axis=1). You can prefent this error message when you adjust the dimensions of the two elements:"
      ],
      "metadata": {
        "id": "xaB4EEqnabyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(4, 5)\n",
        "A / A.sum(axis=1, keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMEf6zbha0xw",
        "outputId": "8fd2aaac-8e60-4777-e9da-7fe9d292e8aa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1000, 0.2000, 0.3000, 0.4000],\n",
              "        [0.1429, 0.1714, 0.2000, 0.2286, 0.2571],\n",
              "        [0.1667, 0.1833, 0.2000, 0.2167, 0.2333],\n",
              "        [0.1765, 0.1882, 0.2000, 0.2118, 0.2235]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3_3bYqPbl16"
      },
      "source": [
        "### Exercise 4:\n",
        "\n",
        "As with an ordinary Python array, we can access the length of a tensor by calling Pythonâ€™s built-in `len()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f374NfPgbfx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c604322-8f0e-4100-b82a-d8f56530c435"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "x = torch.arange(4)\n",
        "len(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbleR1jrbFqW"
      },
      "source": [
        "We now define a tensor `X` of shape `(2, 3, 4)` . What is the output of `len(X)`? Can you analyse the reason for the output? Create a text cell to provide your answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sa_dP5PSa1fX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3436310a-7b51-40c3-dc5c-f83b02148508"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "len(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The length of a tensor refers to the number of elements in the first dimension. In this case our tensor has 2 elements in the first dimension, therefore we get the result for len(x) as 2. The length of a tensor only refers to its first element, on the otherside you have the size of the tensor which is in our case 2x3x4=24."
      ],
      "metadata": {
        "id": "gZa5bzjKDg_J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PemQ00mUdp9Q"
      },
      "source": [
        "### Exercise 5:\n",
        "\n",
        "We have seen examples of PyTorch `Tensor` broadcasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s546kHtZd8Et",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015455bc-7da7-4ab0-814e-4117d226c195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0],\n",
            "        [1],\n",
            "        [2]]) tensor([0, 1, 2, 3, 4])\n"
          ]
        }
      ],
      "source": [
        "a = torch.arange(3).reshape((3, 1))\n",
        "b = torch.arange(5)\n",
        "a.shape, b.shape\n",
        "print(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VlSi8gkd-Jc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8dab6f-a532-44eb-9dff-8972e636b87a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2, 3, 4],\n",
              "        [1, 2, 3, 4, 5],\n",
              "        [2, 3, 4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "a + b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TePM4O49eFWI"
      },
      "source": [
        "Replace the two tensors in the broadcasting mechanism with different shapes of 3 or 4-dimensional tensors. For example `a = torch.empty(5,2,4,1)`,\n",
        "`b = torch.empty(3,1,1)`. Is the result the same as expected? Analyse the results. Come up with 3D or 4D tensors with different sizes at each dimension. Can you come up with the rules with examples which are required for Tensor broadcasting to work? Creat a text block to provide your analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.empty(5,2,4,1)\n",
        "b = torch.empty(3,1,1)\n",
        "# a + b\n",
        "# The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
      ],
      "metadata": {
        "id": "C-XM1mm9Epqs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of the addition of a+b is an error message. Since there are basically three main rules for tensor broadcasting:\n",
        "1.) When you have two tensors with the same amount of dimensions, then the magnitudes of every dimension has to be the same or it has to be one in at least one of the two tensors. The tensors (2, 1, 3) and (1, 2, 3) can be extended, but the tensors (2, 3, 3) and (3, 3, 3) can not be extended, althoug they differ only in one dimension.\n"
      ],
      "metadata": {
        "id": "1973eo3GHgrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.) Rule\n",
        "t1_r1 = torch.rand(2, 1, 3)\n",
        "t2_r1 = torch.rand(1, 2, 3)\n",
        "t3_r1 = torch.rand(2, 3, 3)\n",
        "t4_r1 = torch.rand(3, 3, 3)\n",
        "\n",
        "t1_r1 + t2_r1\n",
        "# t3_r1 + t4_r1\n",
        "# The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0\n",
        "\n",
        "# This combination is also possible: \n",
        "# t1_r1 + t3_r1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-3glB9FKdPC",
        "outputId": "cbb920a9-a3fb-415d-b4cc-2371a6def251"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0443, 1.1988, 0.2744],\n",
              "         [0.9846, 1.3001, 0.5141]],\n",
              "\n",
              "        [[1.1166, 1.1896, 1.1557],\n",
              "         [1.0569, 1.2910, 1.3955]]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.) When two tensors have different numbers of dimensions, then the tensor with fewer dimensions is expanded with the size of 1 to increase the number of dimensions to the same number as the biger tensor.For exaple the the tensors (2, 3, 4) and (3, 4)  can be expanded by one dimension so they become the tensors (2, 3, 4) and (2, 3, 4), now we can add up the tow tensors."
      ],
      "metadata": {
        "id": "ErNWwWWYKjON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.) Rule\n",
        "t1_r2 = torch.rand(2, 3, 4)\n",
        "t2_r2 = torch.rand(3, 4)\n",
        "\n",
        "t1_r2 + t2_r2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZDYtwrQL3Rh",
        "outputId": "91124823-04bf-46fc-a310-d0634a86dff5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.5630, 0.1679, 1.3697, 0.6004],\n",
              "         [1.7121, 1.5562, 0.4634, 0.2782],\n",
              "         [1.0194, 1.4151, 1.6721, 0.7430]],\n",
              "\n",
              "        [[1.5553, 0.7381, 1.6876, 0.9418],\n",
              "         [1.5257, 0.8665, 0.5724, 0.9973],\n",
              "         [0.9505, 1.2825, 1.6990, 1.0011]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.) When a tensor of size 1 is in a certain dimension, it can get expanded along its dimensions to the size of the other tensor. This example is similar to the first one, the tensors (2, 1, 4) and (2, 2, 4) can be combined when the first tensor gets expanded to (2, 2, 4)."
      ],
      "metadata": {
        "id": "-bOnYvu-MBrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.) Rule\n",
        "t1_r3 = torch.rand(2, 1, 4)\n",
        "t2_r3 = torch.rand(2, 2, 4)\n",
        "\n",
        "t1_r3 + t2_r3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEAWFpfCMJPQ",
        "outputId": "6d0ad1c3-f34e-4a25-ef35-d2e7d65e3816"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.6971, 1.3166, 1.1337, 1.3806],\n",
              "         [1.1185, 0.5080, 1.4235, 1.4717]],\n",
              "\n",
              "        [[1.5727, 1.3246, 0.1998, 0.9530],\n",
              "         [1.5949, 1.6226, 1.1201, 0.9108]]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOJzmeyQj_Y-"
      },
      "source": [
        "### Exercise 6:\n",
        "\n",
        "Create a one dimensional tensor `x` containing the numbers `0` to `23` in order.\n",
        "Reshape x in the next code cell to create the following tensor:\n",
        "\n",
        "```\n",
        "tensor([[ 0,  1,  2,  3, 12, 13, 14, 15],\n",
        "             [ 4,  5,  6,  7, 16, 17, 18, 19],\n",
        "             [ 8,  9, 10, 11, 20, 21, 22, 23]])\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a one dimensional tensor with the number 0 to 23 and reshape the tensor\n",
        "tensor_6 = torch.arange(24)\n",
        "print(tensor_6.shape)\n",
        "tensor_6 = tensor_6.reshape(3, -1)\n",
        "\n",
        "# Reorder the numbers by first creating subtensors which have to replace the old \n",
        "# order\n",
        "sub_1 = tensor_6[0, 0:4]\n",
        "sub_2 = tensor_6[0, 4:8]\n",
        "sub_3 = tensor_6[1, 0:4]\n",
        "sub_4 = tensor_6[1, 4:8]\n",
        "sub_5 = tensor_6[2, 0:4]\n",
        "sub_6 = tensor_6[2, 4:8]\n",
        "print(sub_1, sub_1.shape)\n",
        "\n",
        "# Plug in the subtensors\n",
        "row_1 = torch.cat((sub_1, sub_4), dim=-1)\n",
        "row_2 = torch.cat((sub_2, sub_5), dim=-1)\n",
        "row_3 = torch.cat((sub_3, sub_6), dim=-1)\n",
        "final = torch.cat((row_1, row_2, row_3), dim = 0)\n",
        "final = final.reshape(3, -1)\n",
        "\n",
        "# You can do the task more efficient (the first reshape is not necessary), but I\n",
        "# only found out at the end that I had to reorder the sub tensors so that is the\n",
        "# reason why I used this long approach\n",
        "print(final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liibXO5FjarO",
        "outputId": "b2a12ae0-3e6a-414a-d689-c14562e1250f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([24])\n",
            "tensor([0, 1, 2, 3]) torch.Size([4])\n",
            "tensor([[ 0,  1,  2,  3, 12, 13, 14, 15],\n",
            "        [ 4,  5,  6,  7, 16, 17, 18, 19],\n",
            "        [ 8,  9, 10, 11, 20, 21, 22, 23]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAwL3IAdmIze"
      },
      "source": [
        "### Exercise 7:\n",
        "\n",
        "A one-hot vector for an integer $n$ is a vector that has a one in its $n$th slot, and zeros in all other slots. One-hot vectors are used to represent categorical variables in machine learning.\n",
        "\n",
        "Implement a function in the following code cell that creates a 2D PyTorch tensor of one-hot row vectors from a list of Python integers.\n",
        "\n",
        "For example, given a list `[1, 2, 3, 3]` of integers, your function should produce the 2D tensor:\n",
        "\n",
        "```\n",
        "[[0 1 0 0],\n",
        " [0 0 1 0],\n",
        " [0 0 0 1],\n",
        " [0 0 0 1]]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "HhOt0ZBLm7Ul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee99b5d-590f-4cc7-e9b5-e7e426eb775c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
            "        [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
            "        [0, 1, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ],
      "source": [
        "def one_hot_vector(x):\n",
        "  # Get the shape of the tensor\n",
        "  x_dimension = len(x)\n",
        "  y_dimension = max(x)\n",
        "  raw_tensor = torch.zeros((x_dimension, y_dimension + 1)) \n",
        "\n",
        "  # Implement the 1s into the right postion\n",
        "  for i in range(0, len(x)):\n",
        "    raw_tensor[i, x[i]] = 1\n",
        "  \n",
        "  # In the example it looks like that we only have integers in our tensor so \n",
        "  # I change the data type to int64\n",
        "  final_tensor = raw_tensor.to(torch.int64)\n",
        "\n",
        "  return(final_tensor)\n",
        "\n",
        "# test your function \n",
        "x = [1, 2, 3, 3, -1, 0, 8, -2, 1]\n",
        "print(one_hot_vector(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1rvW5Groxjk"
      },
      "source": [
        "### Exercise 8:\n",
        "\n",
        "Use the GPU to accelerate multiplication of the following large 2D PyTorch tensors. First perform the multiplication on CPU. Next perform the computation on GPU. Compare the time required for mulplication on CPU vs GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "htsK3scgpI2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93cd3374-28d9-4b18-e453-0b3a79391d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x device: cpu\n",
            "y device: cpu\n",
            "y device: cuda:0\n",
            "y device: cuda:0\n",
            "1.05070161819458\n",
            "0.04694724082946777\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(1024, 4096)\n",
        "y = torch.rand(4096, 8192)\n",
        "\n",
        "# Get the device\n",
        "print('x device:', x.device)\n",
        "print('y device:', y.device)\n",
        "\n",
        "# 1. Do the multiplication on the CPU:\n",
        "start_cpu = time.time()\n",
        "cpu_result= torch.mm(x, y)\n",
        "end_cpu = time.time()\n",
        "duration_cpu = end_cpu - start_cpu\n",
        "\n",
        "# Change the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "x_gpu = x.to(device)\n",
        "y_gpu = y.to(device)\n",
        "print('y device:', x_gpu.device)\n",
        "print('y device:', y_gpu.device)\n",
        "\n",
        "# 2. Do the multiplication on the GPU:\n",
        "start_gpu = time.time()\n",
        "gpu_result= torch.mm(x_gpu, y_gpu)\n",
        "gpu_result = gpu_result.to(\"cpu\")\n",
        "end_gpu = time.time()\n",
        "duration_gpu = end_gpu - start_gpu\n",
        "\n",
        "# The result for CPU multiplication:\n",
        "print(duration_cpu)\n",
        "\n",
        "# The result for GPU multiplication:\n",
        "print(duration_gpu)\n",
        "\n",
        "# The GPU calculation is faster than the CPU calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-8RSk8_rCAF"
      },
      "source": [
        "### Exercise 9: \n",
        "\n",
        "Create a function to compute the number of negative values in a tensor.\n",
        "Your code should not use any loops. After implementing the function, test your function with input PyTorch tensors of different shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "ATNJZuj8rZO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8f780a-1544-4c50-8b69-bf2cd49ada11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "2\n",
            "0\n",
            "12\n"
          ]
        }
      ],
      "source": [
        "def negative_value_count(x):\n",
        "  # Count the negative values \n",
        "  n_neg_values = torch.sum(x < 0).item()\n",
        "  return(n_neg_values)\n",
        "\n",
        "test_1 = torch.arange(8)\n",
        "test_2 = torch.arange(-2, 4)\n",
        "test_3 = torch.ones(3,4)\n",
        "test_4 = torch.full((3, 4), -1)\n",
        "\n",
        "# All the tests give me the correct answers \n",
        "print(negative_value_count(test_1))\n",
        "print(negative_value_count(test_2))\n",
        "print(negative_value_count(test_3))\n",
        "print(negative_value_count(test_4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_0MpGnscrH"
      },
      "source": [
        "### Exercise 10:\n",
        "\n",
        "Create a function which returns the copy of the input tensor but with maximum value along each column set to `-1`.\n",
        "\n",
        "  For example:\n",
        "\n",
        "```\n",
        " x = torch.tensor([[\n",
        "        [12, 21, 1],\n",
        "        [ 4,  7,  20]\n",
        "      ]])\n",
        "```\n",
        "\n",
        "\n",
        "  Then `y = negative_max_column(x)` should be:\n",
        " \n",
        "  ```\n",
        "torch.tensor([\n",
        "    [-1, -1, 1],\n",
        "    [2,  5,  -1]\n",
        "  ])\n",
        "```\n",
        "\n",
        "\n",
        "Your code should not use any loops. Test your function with some examples."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def negative_max_column(x):\n",
        "\n",
        "  # Get the index of the highest number in each row and replace it with -1\n",
        "  x[x == torch.max(x, dim=1, keepdim=True)[0]] = -1\n",
        "\n",
        "  # Return the final Tensor\n",
        "  return(x) \n",
        "\n",
        "# Test the function\n",
        "x = torch.tensor([\n",
        "        [-5, 6, 0],\n",
        "        [20, 8, 100],\n",
        "        [4, 4, 1]\n",
        "      ])\n",
        "t1_r2 = torch.rand(2, 3, 4)\n",
        "print(negative_max_column(t1_r2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V84Zj97Dooyj",
        "outputId": "0c443ae9-97e3-4624-8093-949347a8ad27"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.6370,  0.3068,  0.5301,  0.3459],\n",
            "         [-1.0000,  0.1440, -1.0000, -1.0000],\n",
            "         [ 0.1061, -1.0000,  0.5538,  0.4553]],\n",
            "\n",
            "        [[-1.0000,  0.3920,  0.2897,  0.2165],\n",
            "         [ 0.0055,  0.3043, -1.0000, -1.0000],\n",
            "         [ 0.2617, -1.0000,  0.2507,  0.6132]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqTMByiGzY5M"
      },
      "source": [
        "### Exercise 11:\n",
        "\n",
        "Write a function to subtract the mean of each row from a 2D tensor. Your code should not use any loops. Test your function with some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "iFSxG08czz-3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6224961d-476c-43b5-c7a8-c40f0e6d86d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -1.,  -1.,   2.],\n",
              "        [  0., -50.,  50.],\n",
              "        [  1.,   1.,  -2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "def subtract_row_mean(x):\n",
        "\n",
        "  # Convert the tensor input into torch.float32\n",
        "  x = x.to(torch.float32)\n",
        "\n",
        "  # Get the mean\n",
        "  means = torch.mean(x, dim=1, keepdim=True)\n",
        "  result = x - means\n",
        "\n",
        "  # Return the results\n",
        "  return result\n",
        "\n",
        "# Test the function\n",
        "x = torch.tensor([\n",
        "        [3, 3, 6],\n",
        "        [50, 0, 100],\n",
        "        [4, 4, 1]\n",
        "      ])\n",
        "subtract_row_mean(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwTLS7_P54eZ"
      },
      "source": [
        "### Exercise 12:\n",
        "\n",
        "Feed a tensor with 3 or more dimensions to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape? Explain in a block below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "vhnPvgz86AsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f772eb92-316e-47e2-a588-1eccd6994acf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.881943016134134 5.385164807134504\n"
          ]
        }
      ],
      "source": [
        "test_tensor_1 = torch.tensor([[[1, 2, 3], [4, 5, 6], [7, 8, 9]]])\n",
        "test_tensor_2 = torch.tensor([[[2], [3], [4]]])\n",
        "\n",
        "test_1 = np.linalg.norm(test_tensor_1)\n",
        "test_2 = np.linalg.norm(test_tensor_2)\n",
        "\n",
        "print(test_1, test_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function linalg.norm gives you the norm of the tensor. For the test_tensor_2 you can also compute it by hand by the following formular:"
      ],
      "metadata": {
        "id": "0C2pMXg12UTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_tensor_2 = math.sqrt(2**2 + 3**2 + 4**2)\n",
        "\n",
        "print(norm_tensor_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-M_4GUg29t_",
        "outputId": "0c1fd455-9fe4-4dcd-8f49-0301f14e7128"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.385164807134504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON8rugsexuOY"
      },
      "source": [
        "### Exercise 13: \n",
        "\n",
        "Implement a function that takes in two $2 \\mathrm{D}$ tensors $A$ and $B$ and returns the column sum of A multiplied by the sum of all the elmements of $\\boldsymbol{B}$, i.e., a scalar, e.g.,\n",
        "If $A=\\left[\\begin{array}{ll}1 & 1 \\\\ 1 & 1\\end{array}\\right]$ and $B=\\left[\\begin{array}{lll}1 & 2 & 3 \\\\ 1 & 2 & 3\\end{array}\\right]$ then $O u t=\\left[\\begin{array}{ll}2 & 2\\end{array}\\right] \\cdot 12=\\left[\\begin{array}{ll}24 & 24\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "S5YVpsJLx9fO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb4efac5-dd5e-41ac-935a-f087508dd9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([24, 24])\n"
          ]
        }
      ],
      "source": [
        "def calculating(tens_1, tens_2):\n",
        "  col_sum = tens_1.sum(axis=0)\n",
        "  all_sum = tens_2.sum()\n",
        "  result = col_sum * all_sum\n",
        "  return result\n",
        "\n",
        "# Testing\n",
        "tens_1 = torch.tensor([[1, 1], [1, 1]])\n",
        "tens_2 = torch.tensor([[1, 2, 3], [1, 2, 3]])\n",
        "print(calculating(tens_1, tens_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9sRzXzKx_UP"
      },
      "source": [
        "### Exercise 14:\n",
        "\n",
        "Implement a function that takes in a square matrix $A$ and returns a $2 D$ tensor consisting of a flattened $A$ with the index of each element appended to this tensor in the row dimension, e.g.,\n",
        "If $A=\\left[\\begin{array}{cc}2 & 3 \\\\ -1 & 10\\end{array}\\right]$ then $O u t=\\left[\\begin{array}{cc}0 & 2 \\\\ 1 & 3 \\\\ 2 & -1 \\\\ 3 & 10\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "gv0g3w5_yZzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2752d0-2138-4c75-9e13-947b7c02b8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 2.],\n",
            "        [1., 3.],\n",
            "        [2., 4.],\n",
            "        [3., 1.],\n",
            "        [4., 2.],\n",
            "        [5., 0.]])\n"
          ]
        }
      ],
      "source": [
        "def flatting(tens):\n",
        "  # Create a new empty flat tensor\n",
        "  length = tens.size(0) * tens.size(1)\n",
        "  flat_tensor = torch.zeros(length, 2)\n",
        "\n",
        "  # Reshape the input tensor\n",
        "  new_tensor = tens.reshape(-1, 1)\n",
        "\n",
        "  # Fill the flat_tensor with the right numbers\n",
        "  for i in range(0, len(new_tensor)):\n",
        "    flat_tensor[i, 0] = i\n",
        "    flat_tensor[i, 1] = new_tensor[i]\n",
        "\n",
        "  # Return the flat tensor\n",
        "  return flat_tensor\n",
        "\n",
        "# Test the function\n",
        "tens_test = torch.tensor([[2, 3, 4], [1, 2, 0]])\n",
        "print(flatting(tens_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prm8Wx65xqC5"
      },
      "source": [
        "### Exercise 15:\n",
        "\n",
        "Implement a function that takes in two $2 D$ tensors $A$ and $B$. If the shapes allow it, this function returns the elementwise sum of $A$-shaped $B$, and $B$; else this function returns a 1D tensor that is the concatenation of the two tensors, e.g.,\n",
        "If $A=\\left[\\begin{array}{cc}1 & -1 \\\\ -1 & 3\\end{array}\\right]$ and $B=\\left[\\begin{array}{llll}2 & 3 & 0 & 2\\end{array}\\right]$ then $O u t=\\left[\\begin{array}{cc}3 & 2 \\\\ -1 & 5\\end{array}\\right]$\n",
        "If $A=\\left[\\begin{array}{cc}1 & -1 \\\\ -1 & 3\\end{array}\\right]$ and $B=\\left[\\begin{array}{ccc}2 & 3 & 0\\end{array}\\right]$ then $O u t=\\left[\\begin{array}{ccccccc}1 & -1 & -1 & 3 & 2 & 3 & 0\\end{array}\\right]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "di2J13g_zB9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872f646b-6d37-4d0c-ef0a-ab3a93a5f291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 2, 0],\n",
            "        [1, 9, 9]])\n",
            "tensor([[ 1., -1.,  0., -1.,  3.,  8.,  2.,  3.,  0.,  2.,  6.]])\n"
          ]
        }
      ],
      "source": [
        "def adding_tensors(tens_a, tens_b):\n",
        "\n",
        "  # Check if elementwise sum is possible\n",
        "  a_shape = tens_a.shape[0] * tens_a.shape[1]\n",
        "  new_shape_x = tens_a.shape[0]\n",
        "  new_shape_y = tens_a.shape[1]\n",
        "  b_shape = tens_b.shape[0] * tens_b.shape[1]\n",
        "  total_elements = tens_a.numel() + tens_b.numel()\n",
        "\n",
        "  # First we look at the case where can sum up the tensors\n",
        "  if a_shape == b_shape:\n",
        "    new_tens_a = tens_a.reshape(-1, 1)\n",
        "    new_tens_b = tens_b.reshape(-1, 1)\n",
        "    tens_sum = torch.zeros_like(new_tens_a)\n",
        "\n",
        "    # Fill in the new sum tensor\n",
        "    for i in range(0, len(tens_sum)):\n",
        "      tens_sum[i, 0] = new_tens_a[i, 0] + new_tens_b[i, 0]\n",
        "\n",
        "    # Reshape the sum tensor into the\n",
        "    final_tensor = tens_sum.view(new_shape_x, new_shape_y)\n",
        "\n",
        "    # Return the final tensor\n",
        "    return final_tensor \n",
        "\n",
        "  # Now we look at the case where we can not sum up the tensors\n",
        "  else:\n",
        "    tens_concat = torch.zeros(1, total_elements)\n",
        "    new_tens_a = tens_a.reshape(-1, 1)\n",
        "    new_tens_b = tens_b.reshape(-1, 1)\n",
        "\n",
        "    # Fill in the concated tensor\n",
        "    for i in range(0, len(new_tens_a)):\n",
        "      tens_concat[0, i] = new_tens_a[i, 0]\n",
        "\n",
        "    # Fille the rest of the tensor\n",
        "    for j in range(0, len(new_tens_b)):\n",
        "      tens_concat[0, j+len(new_tens_a)] = new_tens_b[j, 0]\n",
        "\n",
        "    # Return the concat tensor\n",
        "    return tens_concat\n",
        "    # You could have done this moreeasily mwith the torch.cat function but first\n",
        "    # fuoond this function for the next exercise 16 \n",
        "\n",
        "# Testing\n",
        "tens_a = torch.tensor([[1, -1, 0], [-1, 3, 8]])\n",
        "tens_b1 = torch.tensor([[2, 3, 0, 2, 6, 1]])\n",
        "tens_b2 = torch.tensor([[2, 3, 0, 2, 6]])\n",
        "print(adding_tensors(tens_a, tens_b1))\n",
        "print(adding_tensors(tens_a, tens_b2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0AW0MgM2IeW"
      },
      "source": [
        "### Exercise 16:\n",
        "\n",
        "You are given a tensor `samples` with 12 sequences of length 15. Adapt the code below to add a `new_sample` to `samples` tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "lP6teQMI2H-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b88a3e0-98d5-4484-a8a7-290dfe1d6445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([12, 15])\n",
            "torch.Size([15])\n",
            "torch.Size([1, 15])\n",
            "tensor([[ 7.3872e-01,  2.3552e-01,  1.0543e+00, -1.0504e+00, -2.1557e+00,\n",
            "         -1.3800e-01,  2.3296e+00, -7.3122e-01,  1.4418e+00, -4.1030e-01,\n",
            "         -1.6334e+00,  1.3518e+00, -8.8716e-01,  6.6167e-01,  2.4278e+00],\n",
            "        [-1.1329e+00, -9.3975e-01,  1.0207e+00,  6.2327e-01, -7.7920e-02,\n",
            "          5.6054e-02,  4.4055e-01,  2.0526e-01, -2.2352e+00,  1.4284e+00,\n",
            "          1.1027e+00, -7.7847e-01,  1.3495e+00,  1.1413e+00,  2.8219e-01],\n",
            "        [ 9.0616e-02,  1.1563e+00, -4.2676e-01, -6.2902e-01, -1.0432e+00,\n",
            "          2.5344e-02,  2.0706e+00,  5.2312e-01, -3.3070e-01, -2.3625e-01,\n",
            "          7.2384e-01, -1.1246e+00,  1.0369e+00, -2.7379e+00, -5.8322e-01],\n",
            "        [ 2.2135e+00, -1.5929e+00,  7.0380e-01,  7.4870e-01,  4.0565e-01,\n",
            "         -5.2897e-01, -3.5793e-01,  1.2909e+00, -2.4296e-01,  2.9967e+00,\n",
            "         -9.8969e-02,  1.1620e+00, -1.7453e-01, -1.4380e-01, -6.5833e-02],\n",
            "        [ 2.3621e-01, -5.0877e-01,  1.9775e+00, -5.4711e-01, -3.8286e-01,\n",
            "         -7.1743e-01,  2.3555e+00, -8.9102e-01, -3.5948e-01, -2.6400e-02,\n",
            "         -1.9345e+00,  1.4724e+00, -1.0643e-01,  5.1077e-01, -3.2082e-01],\n",
            "        [-4.6707e-01,  7.7042e-01, -7.5226e-01,  2.8858e+00,  4.3061e-01,\n",
            "         -9.6654e-02,  2.2762e+00,  9.3110e-01,  4.1347e-02, -9.6452e-02,\n",
            "         -7.0341e-01, -1.1084e-01,  8.8236e-01, -7.6184e-04,  2.6118e-02],\n",
            "        [ 1.4462e+00,  1.5120e+00,  1.4831e-01, -3.7279e-01, -8.4118e-01,\n",
            "          6.6347e-01,  1.4499e+00,  9.2517e-01,  4.0238e-01,  1.5173e-02,\n",
            "          6.2175e-01,  3.1438e-01,  9.2341e-01, -2.5045e-01, -1.9759e+00],\n",
            "        [ 1.2022e+00, -1.0890e+00,  1.6262e-01, -6.3308e-01,  3.0223e-01,\n",
            "         -7.1192e-01, -8.7452e-01,  1.2779e+00, -9.4379e-01,  2.1055e-01,\n",
            "         -6.5677e-01,  5.7120e-01,  1.7892e+00,  1.4414e-02, -1.0882e+00],\n",
            "        [ 2.8622e-02,  1.0665e+00,  6.8455e-01,  5.6591e-01,  4.1725e-01,\n",
            "          9.2170e-01,  1.0842e+00, -3.0506e-01,  3.8092e-01,  2.4404e-01,\n",
            "          8.8250e-01, -1.6839e-02,  9.3983e-01, -4.1167e-01,  1.7185e-01],\n",
            "        [ 2.9167e-01, -5.5796e-01,  7.3908e-01, -1.5620e+00, -3.5506e-02,\n",
            "          3.2178e-01,  9.0814e-01, -1.3770e+00, -8.0943e-01, -1.2833e+00,\n",
            "          6.4510e-01,  4.4808e-01, -1.0678e+00,  1.9356e+00, -1.7306e+00],\n",
            "        [-7.7515e-02,  1.1357e+00, -1.0220e+00,  3.8606e-01, -2.1073e-01,\n",
            "          4.7303e-01,  7.9140e-01, -1.0499e+00, -1.7129e+00, -1.5397e+00,\n",
            "         -1.4999e+00,  1.6733e+00,  5.8653e-01,  1.4270e-01,  3.1639e-01],\n",
            "        [ 4.4455e-01, -4.6159e-02,  3.2523e-01, -9.5802e-01,  5.5107e-01,\n",
            "          6.0539e-01, -1.4119e+00,  6.6644e-01,  1.4891e+00, -9.0589e-01,\n",
            "          2.0109e-04,  3.2108e-01, -4.8144e-01, -8.3637e-01,  6.7685e-02],\n",
            "        [-1.0042e-01,  8.7380e-01, -1.3176e+00,  7.0050e-01, -2.6707e+00,\n",
            "         -1.5805e-01,  7.5449e-01, -3.4631e+00, -7.5521e-01, -8.9347e-01,\n",
            "         -5.8170e-01, -8.7212e-01,  3.4847e-01,  8.9868e-01,  9.9165e-01]])\n",
            "torch.Size([13, 15])\n"
          ]
        }
      ],
      "source": [
        "samples = torch.randn(size=(12, 15))\n",
        "new_sample = torch.randn(size=(15,))\n",
        "\n",
        "# Have a look at the shapes of the tensors\n",
        "print(samples.shape)\n",
        "print(new_sample.shape)\n",
        "\n",
        "# Add a dimension to the second tensor\n",
        "new_sample = new_sample.unsqueeze(0)\n",
        "print(new_sample.shape)\n",
        "\n",
        "# Now we can simply add the second tensor to the first one\n",
        "samples = torch.cat((samples, new_sample), dim=0)\n",
        "print(samples)\n",
        "print(samples.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHbGP9G6zqp5"
      },
      "source": [
        "### Exercise 17:\n",
        "\n",
        "Suppose you have a tensor `images_tensor` containing  a batch of `n_batch` number of images of resolution: 30x30 pixels. `images_tensor` is thus of shape `(n_batch, 1, 30, 30)`\n",
        "\n",
        "Write a function `flatten_images` that convert `images_tensor` into a tensor containing flattened images (i.e. a tensor of shape: `(n_batch, 1*30*30)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "l-nkDUOV0qEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fac8d13e-d87b-42fb-f650-d5a660999195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.9963, -1.0539,  1.1016,  ...,  1.1888,  0.7153, -0.2652],\n",
            "        [-0.4327,  0.4803, -0.1825,  ...,  0.2880, -0.4013, -0.0792],\n",
            "        [ 1.0705, -1.3218, -0.6981,  ..., -1.3065, -0.7787,  1.0327],\n",
            "        [-0.2615, -1.6786,  0.8539,  ...,  0.0752,  1.5498,  0.4356],\n",
            "        [ 0.6643,  0.8476,  0.2236,  ..., -0.4234,  1.1771, -0.3973]]) torch.Size([5, 900])\n"
          ]
        }
      ],
      "source": [
        "# At first I create an examle tensor images_tensor with n_batch = 5\n",
        "n_batch = 5\n",
        "images_tensor = torch.randn(n_batch, 1, 30, 30)\n",
        "\n",
        "# Have a look at the example tensor\n",
        "#print(images_tensor, images_tensor.shape)\n",
        "\n",
        "# write the function flatten_images\n",
        "def flatten_images(im):\n",
        "\n",
        "  # Get the number of batches\n",
        "  number_batches = images_tensor.shape[0]\n",
        "\n",
        "  # Get the size of the second dimension, all pixels per batch in one row\n",
        "  number_pixels = images_tensor.shape[1] * images_tensor.shape[2] * images_tensor.shape[3]\n",
        "\n",
        "  # Use the view method to transform the tensor\n",
        "  flatten_tensor = images_tensor.view(number_batches, number_pixels)\n",
        "\n",
        "  # Now every picture has its own row \n",
        "  return flatten_tensor\n",
        "\n",
        "# Test the function\n",
        "print(flatten_images(images_tensor), flatten_images(images_tensor).shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}